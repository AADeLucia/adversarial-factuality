task:
  type: peft-sft
  model_name: "gpt2"
  peft_config:
    r: 8
    lora_alpha: 8
    target_modules:
      - "c_attn"
    fan_in_fan_out: true
    lora_dropout: 0.1
    bias: "none"
  output_dir: "ckpt/peft-gpt2-${SPLIT}"
  train_data_path: "data/generated-samples/sampled_train-${SPLIT}.jsonl"
  eval_data_path: "data/generated-samples/sampled_dev-${SPLIT}.jsonl"
  test_data_path: "data/generated-samples/sampled_test-${SPLIT}.jsonl"
  num_train_epochs: 20
  batch_size: 64
  learning_rate: 0.001
  is_chat_model: false
  load_in_8bit: false